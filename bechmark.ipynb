{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python378jvsc74a57bd0d7b42af76f8dae19ba19cf9f045b521d39bc89019013b1af4b96f374b1cb9eee",
   "display_name": "Python 3.7  ('graphBin': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport\n",
    "pyximport.install()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import dgl\n",
    "import higra as hg\n",
    "import higra as hg\n",
    "from functools import partial\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "import DRBin\n",
    "import DRBin.utils as _vambtools\n",
    "from DRBin.models import DGI, LogReg\n",
    "from DRBin import process\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_cluster import radius_graph\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.sparse as sp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/test_data/contigs.fna', 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = DRBin.parsecontigs.read_contigs(filehandle)\n",
    "rpkms = np.load('test_data/abundance.npz')\n",
    "rpkms=rpkms['arr_0']\n",
    "#vae = DRBin.encode.VAE(nsamples=rpkms.shape[1],cuda=True)\n",
    "#dataloader, mask = DRBin.encode.make_dataloader(rpkms, tnfs)\n",
    "latent = np.loadtxt('/test_data/urog_latent.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.zeros(latent.shape[0])\n",
    "latent = torch.tensor(latent)\n",
    "edge_index = radius_graph(latent, r=10, loop=False)\n",
    "u, v = edge_index[0], edge_index[1]\n",
    "g = dgl.graph((u, v))\n",
    "bg = dgl.to_bidirected(g)\n",
    "knn_graph = bg.adj(scipy_fmt='csr')\n",
    "#sp.save_npz('test_data/urogG.npz', knn_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 5000\n",
    "patience = 20\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.3\n",
    "hid_units = 32\n",
    "sparse = True\n",
    "nonlinearity = 'prelu' # special name to separate parameters\n",
    "features = latent\n",
    "adj = sp.load_npz('/test_data/KNN_graph.npz')\n",
    "\n",
    "features = sp.csr_matrix(features)\n",
    "features, _ = process.preprocess_features(features)\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "if sparse:\n",
    "    sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "else:\n",
    "    adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "features = torch.FloatTensor(features[np.newaxis])\n",
    "if not sparse:\n",
    "    adj = torch.FloatTensor(adj[np.newaxis])\n",
    "model = DGI(ft_size, hid_units, nonlinearity)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    if sparse:\n",
    "        sp_adj = sp_adj.cuda()\n",
    "    else:\n",
    "        adj = adj.cuda()\n",
    "b_xent = nn.BCEWithLogitsLoss()\n",
    "xent = nn.CrossEntropyLoss()\n",
    "cnt_wait = 0\n",
    "best = 1e9\n",
    "best_t = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    idx = np.random.permutation(nb_nodes)\n",
    "    shuf_fts = features[:, idx, :]\n",
    "\n",
    "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
    "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
    "    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        shuf_fts = shuf_fts.cuda()\n",
    "        lbl = lbl.cuda()\n",
    "    \n",
    "    logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "\n",
    "    loss = b_xent(logits, lbl)\n",
    "\n",
    "    #print('Loss:', loss)\n",
    "\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        best_t = epoch\n",
    "        cnt_wait = 0\n",
    "        torch.save(model.state_dict(), '/test_data/best_dgi.pkl')\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "model.load_state_dict(torch.load('/test_data/best_dgi.pkl'))\n",
    "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
    "embeds = embeds.squeeze(-3).cpu()\n",
    "embeds = embeds.numpy()\n",
    "\n",
    "a = 0.73\n",
    "#get the final vector for clustering\n",
    "X = (a * features + (1 - a) * embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "from DRBin.ultrametric.optimization import UltrametricFitting\n",
    "from DRBin.ultrametric.data import load_datasets, show_datasets\n",
    "from DRBin.ultrametric.graph import build_graph, show_graphs\n",
    "from DRBin.ultrametric.utils import Experiments\n",
    "from DRBin.ultrametric.evaluation import eval_clustering\n",
    "\n",
    "from DRBin.ultrametric.loss import loss_closest, loss_closest_and_cluster_size, make_triplets, loss_closest_and_triplet, loss_dasgupta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    'average': lambda X, graph, edge_weights: hg.binary_partition_tree_average_linkage(graph, edge_weights),\n",
    "    'ward': lambda X, graph, edge_weights: hg.binary_partition_tree_ward_linkage(graph, X)\n",
    "}\n",
    "\n",
    "n_clusters = 10000\n",
    "A = build_graph(X, 'mst')\n",
    "graph, edge_weights = hg.adjacency_matrix_2_undirected_graph(A)\n",
    "method = methods['average']\n",
    "loss = partial(loss_closest_and_cluster_size, top_nodes=10)\n",
    "optim = UltrametricFitting(500, 0.1, loss)\n",
    "ultrametric = optim.fit(graph, edge_weights)\n",
    "hierarchy = hg.bpt_canonical(graph, ultrametric)\n",
    "Z = hg.binary_hierarchy_to_scipy_linkage_matrix(*hierarchy)\n",
    "y_prediction = fcluster(Z, n_clusters, criterion='maxclust') - 1\n",
    "np.savetxt('result.txt', y_prediction, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.loadtxt('test_data/result.txt', dtype=int)\n",
    "filtered_labels = [n for (n,m) in zip(contignames, mask) if m]\n",
    "result2 = dict()\n",
    "for i in range(57762):\n",
    "    result2[result[i]] = filtered_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "cluster = dict()\n",
    "cluster = collections.defaultdict(set)\n",
    "for i in range(57762):\n",
    "    cluster[result2[result[i]]].add(filtered_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterclusters(clusters, lengthof):\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 200000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(DRBin.utils.binsplit(cluster, 'C'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(cluster))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open('test_data/cluster.tsv', 'w') as file:\n",
    "    DRBin.utils.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "with open('test_data/contigs.fna', 'rb') as file:\n",
    "    fastadict = DRBin.utils.loadfasta(file, keep=keptcontigs)\n",
    "    \n",
    "bindir = 'test_data/result/bins'\n",
    "DRBin.utils.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load in the Reference\n",
    "reference_path = 'test_data/reference.tsv'\n",
    "\n",
    "!head $reference_path # show first 10 lines of reference file\n",
    "\n",
    "with open(reference_path) as reference_file:\n",
    "    reference = DRBin.benchmark.Reference.from_file(reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_path = 'test_data/taxonomy.tsv'\n",
    "\n",
    "!head $taxonomy_path # show first 10 lines of reference file\n",
    "\n",
    "with open(taxonomy_path) as taxonomy_file:\n",
    "    reference.load_tax_file(taxonomy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DRBin bins:')\n",
    "for rank in DRBin_bins.summary():\n",
    "    print('\\t'.join(map(str, rank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}