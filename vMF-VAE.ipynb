{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as _np\n",
    "import torch as _torch\n",
    "_torch.manual_seed(0)\n",
    "\n",
    "from math import log as _log\n",
    "import math\n",
    "\n",
    "from torch import nn as _nn\n",
    "from torch.optim import Adam as _Adam\n",
    "from torch.nn.functional import softmax as _softmax\n",
    "from torch.utils.data import DataLoader as _DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset as _TensorDataset\n",
    "\n",
    "from DRBin.calculate_graph import *\n",
    "from DRBin.eval import *\n",
    "import DRBin\n",
    "import DRBin.utils\n",
    "from DRBin.my_cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_inplace_maskarray(array, mask):\n",
    "    \"\"\"In-place masking of a Numpy array, i.e. if `mask` is a boolean mask of same\n",
    "    length as `array`, then array[mask] == numpy_inplace_maskarray(array, mask),\n",
    "    but does not allocate a new array.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(mask) != len(array):\n",
    "        raise ValueError('Lengths of array and mask must match')\n",
    "    elif len(array.shape) != 2:\n",
    "        raise ValueError('Can only take a 2 dimensional-array.')\n",
    "\n",
    "    uints = _np.frombuffer(mask, dtype=_np.uint8)\n",
    "    index = _overwrite_matrix(array, uints)\n",
    "    array.resize((index, array.shape[1]), refcheck=False)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(array, axis=None, inplace=False):\n",
    "    \"\"\"Calculates zscore for an array. A cheap copy of scipy.stats.zscore.\n",
    "    Inputs:\n",
    "        array: Numpy array to be normalized\n",
    "        axis: Axis to operate across [None = entrie array]\n",
    "        inplace: Do not create new array, change input array [False]\n",
    "    Output:\n",
    "        If inplace is True: None\n",
    "        else: New normalized Numpy-array\"\"\"\n",
    "\n",
    "    if axis is not None and axis >= array.ndim:\n",
    "        raise _np.AxisError('array only has {} axes'.format(array.ndim))\n",
    "\n",
    "    if inplace and not _np.issubdtype(array.dtype, _np.floating):\n",
    "        raise TypeError('Cannot convert a non-float array to zscores')\n",
    "\n",
    "    mean = array.mean(axis=axis)\n",
    "    std = array.std(axis=axis)\n",
    "\n",
    "    if axis is None:\n",
    "        if std == 0:\n",
    "            std = 1 # prevent divide by zero\n",
    "\n",
    "    else:\n",
    "        std[std == 0.0] = 1 # prevent divide by zero\n",
    "        shape = tuple(dim if ax != axis else 1 for ax, dim in enumerate(array.shape))\n",
    "        mean.shape, std.shape = shape, shape\n",
    "\n",
    "    if inplace:\n",
    "        array -= mean\n",
    "        array /= std\n",
    "        return None\n",
    "    else:\n",
    "        return (array - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _torch.__version__ < '0.4':\n",
    "    raise ImportError('PyTorch version must be 0.4 or newer')\n",
    "\n",
    "def make_dataloader(rpkm, tnf, batchsize=256, destroy=False, cuda=True):\n",
    "    \"\"\"Create a DataLoader and a contig mask from RPKM and TNF.\n",
    "    The dataloader is an object feeding minibatches of contigs to the VAE.\n",
    "    The data are normalized versions of the input datasets, with zero-contigs,\n",
    "    i.e. contigs where a row in either TNF or RPKM are all zeros, removed.\n",
    "    The mask is a boolean mask designating which contigs have been kept.\n",
    "    Inputs:\n",
    "        rpkm: RPKM matrix (N_contigs x N_samples)\n",
    "        tnf: TNF matrix (N_contigs x N_TNF)\n",
    "        batchsize: Starting size of minibatches for dataloader\n",
    "        destroy: Mutate rpkm and tnf array in-place instead of making a copy.\n",
    "        cuda: Pagelock memory of dataloader (use when using GPU acceleration)\n",
    "    Outputs:\n",
    "        DataLoader: An object feeding data to the VAE\n",
    "        mask: A boolean mask of which contigs are kept\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(rpkm, _np.ndarray) or not isinstance(tnf, _np.ndarray):\n",
    "        raise ValueError('TNF and RPKM must be Numpy arrays')\n",
    "\n",
    "    if batchsize < 1:\n",
    "        raise ValueError('Minimum batchsize of 1, not {}'.format(batchsize))\n",
    "\n",
    "    if len(rpkm) != len(tnf):\n",
    "        raise ValueError('Lengths of RPKM and TNF must be the same')\n",
    "\n",
    "    if not (rpkm.dtype == tnf.dtype == _np.float32):\n",
    "        raise ValueError('TNF and RPKM must be Numpy arrays of dtype float32')\n",
    "\n",
    "    mask = tnf.sum(axis=1) != 0\n",
    "\n",
    "    # If multiple samples, also include nonzero depth as requirement for accept\n",
    "    # of sequences\n",
    "    if rpkm.shape[1] > 1:\n",
    "        depthssum = rpkm.sum(axis=1)\n",
    "        mask &= depthssum != 0\n",
    "        depthssum = depthssum[mask]\n",
    "\n",
    "    if mask.sum() < batchsize:\n",
    "        raise ValueError('Fewer sequences left after filtering than the batch size.')\n",
    "\n",
    "    if destroy:\n",
    "        rpkm = numpy_inplace_maskarray(rpkm, mask)\n",
    "        tnf = numpy_inplace_maskarray(tnf, mask)\n",
    "    else:\n",
    "        # The astype operation does not copy due to \"copy=False\", but the masking\n",
    "        # operation does.\n",
    "        rpkm = rpkm[mask].astype(_np.float32, copy=False)\n",
    "        tnf = tnf[mask].astype(_np.float32, copy=False)\n",
    "\n",
    "    # If multiple samples, normalize to sum to 1, else zscore normalize\n",
    "    if rpkm.shape[1] > 1:\n",
    "        rpkm /= depthssum.reshape((-1, 1))\n",
    "    else:\n",
    "        zscore(rpkm, axis=0, inplace=True)\n",
    "\n",
    "    # Normalize arrays and create the Tensors (the tensors share the underlying memory)\n",
    "    # of the Numpy arrays\n",
    "    zscore(tnf, axis=0, inplace=True)\n",
    "    depthstensor = _torch.from_numpy(rpkm)\n",
    "    tnftensor = _torch.from_numpy(tnf)\n",
    "\n",
    "    # Create dataloader\n",
    "    n_workers = 4 if cuda else 1\n",
    "    dataset = _TensorDataset(depthstensor, tnftensor)\n",
    "    dataloader = _DataLoader(dataset=dataset, batch_size=batchsize, drop_last=True,\n",
    "                             shuffle=True, num_workers=n_workers, pin_memory=cuda)\n",
    "\n",
    "    return dataloader, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "from numbers import Number\n",
    "\n",
    "\n",
    "class IveFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, v, z):\n",
    "\n",
    "        assert isinstance(v, Number), \"v must be a scalar\"\n",
    "\n",
    "        self.save_for_backward(z)\n",
    "        self.v = v\n",
    "        z_cpu = z.data.cpu().numpy()\n",
    "\n",
    "        if np.isclose(v, 0):\n",
    "            output = scipy.special.i0e(z_cpu, dtype=z_cpu.dtype)\n",
    "        elif np.isclose(v, 1):\n",
    "            output = scipy.special.i1e(z_cpu, dtype=z_cpu.dtype)\n",
    "        else:  #  v > 0\n",
    "            output = scipy.special.ive(v, z_cpu, dtype=z_cpu.dtype)\n",
    "        #         else:\n",
    "        #             print(v, type(v), np.isclose(v, 0))\n",
    "        #             raise RuntimeError('v must be >= 0, it is {}'.format(v))\n",
    "\n",
    "        return torch.Tensor(output).to(z.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        z = self.saved_tensors[-1]\n",
    "        return (\n",
    "            None,\n",
    "            grad_output * (ive(self.v - 1, z) - ive(self.v, z) * (self.v + z) / z),\n",
    "        )\n",
    "\n",
    "\n",
    "class Ive(torch.nn.Module):\n",
    "    def __init__(self, v):\n",
    "        super(Ive, self).__init__()\n",
    "        self.v = v\n",
    "\n",
    "    def forward(self, z):\n",
    "        return ive(self.v, z)\n",
    "\n",
    "\n",
    "ive = IveFunction.apply\n",
    "\n",
    "\n",
    "##########\n",
    "# The below provided approximations were provided in the\n",
    "# respective source papers, to improve the stability of\n",
    "# the Bessel fractions.\n",
    "# I_(v/2)(k) / I_(v/2 - 1)(k)\n",
    "\n",
    "# source: https://arxiv.org/pdf/1606.02008.pdf\n",
    "def ive_fraction_approx(v, z):\n",
    "    # I_(v/2)(k) / I_(v/2 - 1)(k) >= z / (v-1 + ((v+1)^2 + z^2)^0.5\n",
    "    return z / (v - 1 + torch.pow(torch.pow(v + 1, 2) + torch.pow(z, 2), 0.5))\n",
    "\n",
    "\n",
    "# source: https://arxiv.org/pdf/1902.02603.pdf\n",
    "def ive_fraction_approx2(v, z, eps=1e-20):\n",
    "    def delta_a(a):\n",
    "        lamb = v + (a - 1.0) / 2.0\n",
    "        return (v - 0.5) + lamb / (\n",
    "            2 * torch.sqrt((torch.pow(lamb, 2) + torch.pow(z, 2)).clamp(eps))\n",
    "        )\n",
    "\n",
    "    delta_0 = delta_a(0.0)\n",
    "    delta_2 = delta_a(2.0)\n",
    "    B_0 = z / (\n",
    "        delta_0 + torch.sqrt((torch.pow(delta_0, 2) + torch.pow(z, 2))).clamp(eps)\n",
    "    )\n",
    "    B_2 = z / (\n",
    "        delta_2 + torch.sqrt((torch.pow(delta_2, 2) + torch.pow(z, 2))).clamp(eps)\n",
    "    )\n",
    "\n",
    "    return (B_0 + B_2) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypersphericalUniform(_torch.distributions.Distribution):\n",
    "\n",
    "    support = _torch.distributions.constraints.real\n",
    "    has_rsample = False\n",
    "    _mean_carrier_measure = 0\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, val):\n",
    "        self._device = val if isinstance(val, _torch.device) else _torch.device(val)\n",
    "\n",
    "    def __init__(self, dim, validate_args={}, device=\"cuda\"):\n",
    "        super(HypersphericalUniform, self).__init__(\n",
    "            _torch.Size([dim]), validate_args=validate_args\n",
    "        )\n",
    "        self._dim = dim\n",
    "        self.device = device\n",
    "\n",
    "    def sample(self, shape=_torch.Size()):\n",
    "        output = (\n",
    "            _torch.distributions.Normal(0, 1)\n",
    "            .sample(\n",
    "                (shape if isinstance(shape, _torch.Size) else _torch.Size([shape]))\n",
    "                + _torch.Size([self._dim + 1])\n",
    "            )\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        return output / output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.__log_surface_area()\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return -_torch.ones(x.shape[:-1], device=self.device) * self.__log_surface_area()\n",
    "\n",
    "    def __log_surface_area(self):\n",
    "        if torch.__version__ >= \"1.0.0\":\n",
    "            lgamma = _torch.lgamma(_torch.tensor([(self._dim + 1) / 2]).to(self.device))\n",
    "        else:\n",
    "            lgamma = _torch.lgamma(\n",
    "                _torch.Tensor([(self._dim + 1) / 2], device=self.device)\n",
    "            )\n",
    "        return math.log(2) + ((self._dim + 1) / 2) * math.log(math.pi) - lgamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VonMisesFisher(_torch.distributions.Distribution):\n",
    "\n",
    "    arg_constraints = {\n",
    "        \"loc\": _torch.distributions.constraints.real,\n",
    "        \"scale\": _torch.distributions.constraints.positive,\n",
    "    }\n",
    "    support = _torch.distributions.constraints.real\n",
    "    has_rsample = True\n",
    "    _mean_carrier_measure = 0\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        # option 1:\n",
    "        return self.loc * (\n",
    "            ive(self.__m / 2, self.scale) / ive(self.__m / 2 - 1, self.scale)\n",
    "        )\n",
    "        # option 2:\n",
    "        # return self.loc * ive_fraction_approx(torch.tensor(self.__m / 2), self.scale)\n",
    "        # options 3:\n",
    "        # return self.loc * ive_fraction_approx2(torch.tensor(self.__m / 2), self.scale)\n",
    "\n",
    "    @property\n",
    "    def stddev(self):\n",
    "        return self.scale\n",
    "\n",
    "    def __init__(self, loc, scale, validate_args=None, k=1):\n",
    "        self.dtype = loc.dtype\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.device = loc.device\n",
    "        self.__m = loc.shape[-1]\n",
    "        self.__e1 = (_torch.Tensor([1.0] + [0] * (loc.shape[-1] - 1))).to(self.device)\n",
    "        self.k = k\n",
    "\n",
    "        super().__init__(self.loc.size(), validate_args=validate_args)\n",
    "\n",
    "    def sample(self, shape=_torch.Size()):\n",
    "        with torch.no_grad():\n",
    "            return self.rsample(shape)\n",
    "\n",
    "    def rsample(self, shape=_torch.Size()):\n",
    "        shape = shape if isinstance(shape, _torch.Size) else _torch.Size([shape])\n",
    "\n",
    "        w = (\n",
    "            self.__sample_w3(shape=shape)\n",
    "            if self.__m == 3\n",
    "            else self.__sample_w_rej(shape=shape)\n",
    "        )\n",
    "\n",
    "        v = (\n",
    "            _torch.distributions.Normal(0, 1)\n",
    "            .sample(shape + _torch.Size(self.loc.shape))\n",
    "            .to(self.device)\n",
    "            .transpose(0, -1)[1:]\n",
    "        ).transpose(0, -1)\n",
    "        v = v / v.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        w_ = _torch.sqrt(_torch.clamp(1 - (w ** 2), 1e-10))\n",
    "        x = _torch.cat((w, w_ * v), -1)\n",
    "        z = self.__householder_rotation(x)\n",
    "\n",
    "        return z.type(self.dtype)\n",
    "\n",
    "    def __sample_w3(self, shape):\n",
    "        shape = shape + _torch.Size(self.scale.shape)\n",
    "        u = _torch.distributions.Uniform(0, 1).sample(shape).to(self.device)\n",
    "        self.__w = (\n",
    "            1\n",
    "            + _torch.stack(\n",
    "                [_torch.log(u), _torch.log(1 - u) - 2 * self.scale], dim=0\n",
    "            ).logsumexp(0)\n",
    "            / self.scale\n",
    "        )\n",
    "        return self.__w\n",
    "\n",
    "    def __sample_w_rej(self, shape):\n",
    "        c = _torch.sqrt((4 * (self.scale ** 2)) + (self.__m - 1) ** 2)\n",
    "        b_true = (-2 * self.scale + c) / (self.__m - 1)\n",
    "\n",
    "        # using Taylor approximation with a smooth swift from 10 < scale < 11\n",
    "        # to avoid numerical errors for large scale\n",
    "        b_app = (self.__m - 1) / (4 * self.scale)\n",
    "        s = _torch.min(\n",
    "            _torch.max(\n",
    "                _torch.tensor([0.0], dtype=self.dtype, device=self.device),\n",
    "                self.scale - 10,\n",
    "            ),\n",
    "            _torch.tensor([1.0], dtype=self.dtype, device=self.device),\n",
    "        )\n",
    "        b = b_app * s + b_true * (1 - s)\n",
    "\n",
    "        a = (self.__m - 1 + 2 * self.scale + c) / 4\n",
    "        d = (4 * a * b) / (1 + b) - (self.__m - 1) * math.log(self.__m - 1)\n",
    "\n",
    "        self.__b, (self.__e, self.__w) = b, self.__while_loop(b, a, d, shape, k=self.k)\n",
    "        return self.__w\n",
    "\n",
    "    @staticmethod\n",
    "    def first_nonzero(x, dim, invalid_val=-1):\n",
    "        mask = x > 0\n",
    "        idx = _torch.where(\n",
    "            mask.any(dim=dim),\n",
    "            mask.float().argmax(dim=1).squeeze(),\n",
    "            _torch.tensor(invalid_val, device=x.device),\n",
    "        )\n",
    "        return idx\n",
    "\n",
    "    def __while_loop(self, b, a, d, shape, k=20, eps=1e-20):\n",
    "        #  matrix while loop: samples a matrix of [A, k] samples, to avoid looping all together\n",
    "        b, a, d = [\n",
    "            e.repeat(*shape, *([1] * len(self.scale.shape))).reshape(-1, 1)\n",
    "            for e in (b, a, d)\n",
    "        ]\n",
    "        w, e, bool_mask = (\n",
    "            _torch.zeros_like(b).to(self.device),\n",
    "            _torch.zeros_like(b).to(self.device),\n",
    "            (_torch.ones_like(b) == 1).to(self.device),\n",
    "        )\n",
    "\n",
    "        sample_shape = _torch.Size([b.shape[0], k])\n",
    "        shape = shape + _torch.Size(self.scale.shape)\n",
    "\n",
    "        while bool_mask.sum() != 0:\n",
    "            con1 = _torch.tensor((self.__m - 1) / 2, dtype=_torch.float64)\n",
    "            con2 = _torch.tensor((self.__m - 1) / 2, dtype=_torch.float64)\n",
    "            e_ = (\n",
    "                _torch.distributions.Beta(con1, con2)\n",
    "                .sample(sample_shape)\n",
    "                .to(self.device)\n",
    "                .type(self.dtype)\n",
    "            )\n",
    "\n",
    "            u = (\n",
    "                _torch.distributions.Uniform(0 + eps, 1 - eps)\n",
    "                .sample(sample_shape)\n",
    "                .to(self.device)\n",
    "                .type(self.dtype)\n",
    "            )\n",
    "\n",
    "            w_ = (1 - (1 + b) * e_) / (1 - (1 - b) * e_)\n",
    "            t = (2 * a * b) / (1 - (1 - b) * e_)\n",
    "\n",
    "            accept = ((self.__m - 1.0) * t.log() - t + d) > _torch.log(u)\n",
    "            accept_idx = self.first_nonzero(accept, dim=-1, invalid_val=-1).unsqueeze(1)\n",
    "            accept_idx_clamped = accept_idx.clamp(0)\n",
    "            # we use .abs(), in order to not get -1 index issues, the -1 is still used afterwards\n",
    "            w_ = w_.gather(1, accept_idx_clamped.view(-1, 1))\n",
    "            e_ = e_.gather(1, accept_idx_clamped.view(-1, 1))\n",
    "\n",
    "            reject = accept_idx < 0\n",
    "            accept = ~reject if _torch.__version__ >= \"1.2.0\" else 1 - reject\n",
    "\n",
    "            w[bool_mask * accept] = w_[bool_mask * accept]\n",
    "            e[bool_mask * accept] = e_[bool_mask * accept]\n",
    "\n",
    "            bool_mask[bool_mask * accept] = reject[bool_mask * accept]\n",
    "\n",
    "        return e.reshape(shape), w.reshape(shape)\n",
    "\n",
    "    def __householder_rotation(self, x):\n",
    "        u = self.__e1 - self.loc\n",
    "        u = u / (u.norm(dim=-1, keepdim=True) + 1e-5)\n",
    "        z = x - 2 * (x * u).sum(-1, keepdim=True) * u\n",
    "        return z\n",
    "\n",
    "    def entropy(self):\n",
    "        # option 1:\n",
    "        output = (\n",
    "            -self.scale\n",
    "            * ive(self.__m / 2, self.scale)\n",
    "            / ive((self.__m / 2) - 1, self.scale)\n",
    "        )\n",
    "        # option 2:\n",
    "        # output = - self.scale * ive_fraction_approx(torch.tensor(self.__m / 2), self.scale)\n",
    "        # option 3:\n",
    "        # output = - self.scale * ive_fraction_approx2(torch.tensor(self.__m / 2), self.scale)\n",
    "\n",
    "        return output.view(*(output.shape[:-1])) + self._log_normalization()\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return self._log_unnormalized_prob(x) - self._log_normalization()\n",
    "\n",
    "    def _log_unnormalized_prob(self, x):\n",
    "        output = self.scale * (self.loc * x).sum(-1, keepdim=True)\n",
    "\n",
    "        return output.view(*(output.shape[:-1]))\n",
    "\n",
    "    def _log_normalization(self):\n",
    "        output = -(\n",
    "            (self.__m / 2 - 1) * _torch.log(self.scale)\n",
    "            - (self.__m / 2) * math.log(2 * math.pi)\n",
    "            - (self.scale + _torch.log(ive(self.__m / 2 - 1, self.scale)))\n",
    "        )\n",
    "\n",
    "        return output.view(*(output.shape[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(_nn.Module):\n",
    "    \"\"\"Variational autoencoder, subclass of torch.nn.Module.\n",
    "    Instantiate with:\n",
    "        nsamples: Number of samples in abundance matrix\n",
    "        nhiddens: List of n_neurons in the hidden layers [None=Auto]\n",
    "        nlatent: Number of neurons in the latent layer [32]\n",
    "        alpha: Approximate starting TNF/(CE+TNF) ratio in loss. [None = Auto]\n",
    "        beta: Multiply KLD by the inverse of this value [200]\n",
    "        dropout: Probability of dropout on forward pass [0.2]\n",
    "        cuda: Use CUDA (GPU accelerated training) [False]\n",
    "    vae.trainmodel(dataloader, nepochs batchsteps, lrate, logfile, modelfile)\n",
    "        Trains the model, returning None\n",
    "    vae.encode(self, data_loader):\n",
    "        Encodes the data in the data loader and returns the encoded matrix.\n",
    "    If alpha or dropout is None and there is only one sample, they are set to\n",
    "    0.99 and 0.0, respectively\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nsamples, nhiddens=None, nlatent=32, alpha=None,\n",
    "                 beta=200, dropout=0.2, cuda=False):\n",
    "        if nlatent < 1:\n",
    "            raise ValueError('Minimum 1 latent neuron, not {}'.format(latent))\n",
    "\n",
    "        if nsamples < 1:\n",
    "            raise ValueError('nsamples must be > 0, not {}'.format(nsamples))\n",
    "\n",
    "        # If only 1 sample, we weigh alpha and nhiddens differently\n",
    "        if alpha is None:\n",
    "            alpha = 0.15 if nsamples > 1 else 0.50\n",
    "\n",
    "        if nhiddens is None:\n",
    "            nhiddens = [512, 512] if nsamples > 1 else [256, 256]\n",
    "\n",
    "        if dropout is None:\n",
    "            dropout = 0.2 if nsamples > 1 else 0.0\n",
    "\n",
    "        if any(i < 1 for i in nhiddens):\n",
    "            raise ValueError('Minimum 1 neuron per layer, not {}'.format(min(nhiddens)))\n",
    "\n",
    "        if beta <= 0:\n",
    "            raise ValueError('beta must be > 0, not {}'.format(beta))\n",
    "\n",
    "        if not (0 < alpha < 1):\n",
    "            raise ValueError('alpha must be 0 < alpha < 1, not {}'.format(alpha))\n",
    "\n",
    "        if not (0 <= dropout < 1):\n",
    "            raise ValueError('dropout must be 0 <= dropout < 1, not {}'.format(dropout))\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Initialize simple attributes\n",
    "        self.usecuda = cuda\n",
    "        self.nsamples = nsamples\n",
    "        self.ntnf = 136\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.nhiddens = nhiddens\n",
    "        self.nlatent = nlatent\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize lists for holding hidden layers\n",
    "        self.encoderlayers = _nn.ModuleList()\n",
    "        self.encodernorms = _nn.ModuleList()\n",
    "        self.decoderlayers = _nn.ModuleList()\n",
    "        self.decodernorms = _nn.ModuleList()\n",
    "\n",
    "        # Add all other hidden layers\n",
    "        for nin, nout in zip([self.nsamples + self.ntnf] + self.nhiddens, self.nhiddens):\n",
    "            self.encoderlayers.append(_nn.Linear(nin, nout))\n",
    "            self.encodernorms.append(_nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Latent layers\n",
    "        self.mu = _nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "        self.logsigma = _nn.Linear(self.nhiddens[-1], 1)\n",
    "\n",
    "        # Add first decoding layer\n",
    "        for nin, nout in zip([self.nlatent] + self.nhiddens[::-1], self.nhiddens[::-1]):\n",
    "            self.decoderlayers.append(_nn.Linear(nin, nout))\n",
    "            self.decodernorms.append(_nn.BatchNorm1d(nout))\n",
    "\n",
    "        # Reconstruction (output) layer\n",
    "        self.outputlayer = _nn.Linear(self.nhiddens[0], self.nsamples + self.ntnf)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = _nn.LeakyReLU()\n",
    "        self.softplus = _nn.Softplus()\n",
    "        self.dropoutlayer = _nn.Dropout(p=self.dropout)\n",
    "\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def _encode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        # Hidden layers\n",
    "        for encoderlayer, encodernorm in zip(self.encoderlayers, self.encodernorms):\n",
    "            tensor = encodernorm(self.dropoutlayer(self.relu(encoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        # Latent layers\n",
    "        mu = self.mu(tensor)\n",
    "        mu = mu / mu.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Note: This softplus constrains logsigma to positive. As reconstruction loss pushes\n",
    "        # logsigma as low as possible, and KLD pushes it towards 0, the optimizer will\n",
    "        # always push this to 0, meaning that the logsigma layer will be pushed towards\n",
    "        # negative infinity. This creates a nasty numerical instability in VAMB. Luckily,\n",
    "        # the gradient also disappears as it decreases towards negative infinity, avoiding\n",
    "        # NaN poisoning in most cases. We tried to remove the softplus layer, but this\n",
    "        # necessitates a new round of hyperparameter optimization, and there is no way in\n",
    "        # hell I am going to do that at the moment of writing.\n",
    "        # Also remove needless factor 2 in definition of latent in reparameterize function.\n",
    "        logsigma = self.softplus(self.logsigma(tensor)) + 1\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "    # sample with gaussian noise\n",
    "    def reparameterize(self, mu, logsigma):\n",
    "#         epsilon = _torch.randn(mu.size(0), mu.size(1))\n",
    "\n",
    "#         if self.usecuda:\n",
    "#             epsilon = epsilon.cuda()\n",
    "\n",
    "#         epsilon.requires_grad = True\n",
    "\n",
    "#         # See comment above regarding softplus\n",
    "#         latent = mu + epsilon * _torch.exp(logsigma/2)\n",
    "\n",
    "        q_z = VonMisesFisher(mu, logsigma)\n",
    "        p_z = HypersphericalUniform(self.nlatent - 1)\n",
    "        \n",
    "        latent = q_z.rsample()\n",
    "        \n",
    "        return latent, q_z, p_z\n",
    "\n",
    "    def _decode(self, tensor):\n",
    "        tensors = list()\n",
    "\n",
    "        for decoderlayer, decodernorm in zip(self.decoderlayers, self.decodernorms):\n",
    "            tensor = decodernorm(self.dropoutlayer(self.relu(decoderlayer(tensor))))\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        reconstruction = self.outputlayer(tensor)\n",
    "\n",
    "        # Decompose reconstruction to depths and tnf signal\n",
    "        depths_out = reconstruction.narrow(1, 0, self.nsamples)\n",
    "        tnf_out = reconstruction.narrow(1, self.nsamples, self.ntnf)\n",
    "\n",
    "        # If multiple samples, apply softmax\n",
    "        if self.nsamples > 1:\n",
    "            depths_out = _softmax(depths_out, dim=1)\n",
    "\n",
    "        return depths_out, tnf_out\n",
    "\n",
    "    def forward(self, depths, tnf):\n",
    "        tensor = _torch.cat((depths, tnf), 1)\n",
    "        mu, logsigma = self._encode(tensor)\n",
    "        latent, q_z, p_z = self.reparameterize(mu, logsigma)\n",
    "        depths_out, tnf_out = self._decode(latent)\n",
    "\n",
    "        return depths_out, tnf_out, mu, logsigma, q_z, p_z\n",
    "\n",
    "    def calc_loss(self, depths_in, depths_out, tnf_in, tnf_out, q_z, p_z):\n",
    "        # If multiple samples, use cross entropy, else use SSE for abundance\n",
    "        if self.nsamples > 1:\n",
    "            # Add 1e-9 to depths_out to avoid numerical instability.\n",
    "            ce = - ((depths_out + 1e-9).log() * depths_in).sum(dim=1).mean()\n",
    "            ce_weight = (1 - self.alpha) / _log(self.nsamples)\n",
    "        else:\n",
    "            ce = (depths_out - depths_in).pow(2).sum(dim=1).mean()\n",
    "            ce_weight = 1 - self.alpha\n",
    "\n",
    "        sse = (tnf_out - tnf_in).pow(2).sum(dim=1).mean()\n",
    "        kld = (-q_z.entropy() + p_z.entropy()).mean()\n",
    "        sse_weight = self.alpha / self.ntnf\n",
    "        kld_weight = 1 / (self.nlatent * self.beta)\n",
    "        loss = ce * ce_weight + sse * sse_weight + kld * kld_weight\n",
    "\n",
    "        return loss, ce, sse, kld\n",
    "\n",
    "    def trainepoch(self, data_loader, epoch, optimizer, batchsteps, logfile):\n",
    "        self.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_kldloss = 0\n",
    "        epoch_sseloss = 0\n",
    "        epoch_celoss = 0\n",
    "\n",
    "        if epoch in batchsteps:\n",
    "            data_loader = _DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size * 2,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      num_workers=data_loader.num_workers,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        for depths_in, tnf_in in data_loader:\n",
    "            depths_in.requires_grad = True\n",
    "            tnf_in.requires_grad = True\n",
    "\n",
    "            if self.usecuda:\n",
    "                depths_in = depths_in.cuda()\n",
    "                tnf_in = tnf_in.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            depths_out, tnf_out, mu, logsigma, q_z, p_z = self(depths_in, tnf_in)\n",
    "\n",
    "            loss, ce, sse, kld = self.calc_loss(depths_in, depths_out, tnf_in,\n",
    "                                                  tnf_out, q_z, p_z)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.data.item()\n",
    "            epoch_kldloss += kld.data.item()\n",
    "            epoch_sseloss += sse.data.item()\n",
    "            epoch_celoss += ce.data.item()\n",
    "\n",
    "        if logfile is not None:\n",
    "            print('\\tEpoch: {}\\tLoss: {:.6f}\\tCE: {:.7f}\\tSSE: {:.6f}\\tKLD: {:.4f}\\tBatchsize: {}'.format(\n",
    "                  epoch + 1,\n",
    "                  epoch_loss / len(data_loader),\n",
    "                  epoch_celoss / len(data_loader),\n",
    "                  epoch_sseloss / len(data_loader),\n",
    "                  epoch_kldloss / len(data_loader),\n",
    "                  data_loader.batch_size,\n",
    "                  ), file=logfile)\n",
    "\n",
    "            logfile.flush()\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def encode(self, data_loader):\n",
    "        \"\"\"Encode a data loader to a latent representation with VAE\n",
    "        Input: data_loader: As generated by train_vae\n",
    "        Output: A (n_contigs x n_latent) Numpy array of latent repr.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        new_data_loader = _DataLoader(dataset=data_loader.dataset,\n",
    "                                      batch_size=data_loader.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=1,\n",
    "                                      pin_memory=data_loader.pin_memory)\n",
    "\n",
    "        depths_array, tnf_array = data_loader.dataset.tensors\n",
    "        length = len(depths_array)\n",
    "\n",
    "        # We make a Numpy array instead of a Torch array because, if we create\n",
    "        # a Torch array, then convert it to Numpy, Numpy will believe it doesn't\n",
    "        # own the memory block, and array resizes will not be permitted.\n",
    "        latent = _np.empty((length, self.nlatent), dtype=_np.float32)\n",
    "\n",
    "        row = 0\n",
    "        with _torch.no_grad():\n",
    "            for depths, tnf in new_data_loader:\n",
    "                # Move input to GPU if requested\n",
    "                if self.usecuda:\n",
    "                    depths = depths.cuda()\n",
    "                    tnf = tnf.cuda()\n",
    "\n",
    "                # Evaluate\n",
    "                out_depths, out_tnf, mu, logsigma, q_z, p_z = self(depths, tnf)\n",
    "\n",
    "                if self.usecuda:\n",
    "                    mu = mu.cpu()\n",
    "\n",
    "                latent[row: row + len(mu)] = mu\n",
    "                row += len(mu)\n",
    "\n",
    "        assert row == length\n",
    "        return latent\n",
    "\n",
    "    def save(self, filehandle):\n",
    "        \"\"\"Saves the VAE to a path or binary opened file. Load with VAE.load\n",
    "        Input: Path or binary opened filehandle\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        state = {'nsamples': self.nsamples,\n",
    "                 'alpha': self.alpha,\n",
    "                 'beta': self.beta,\n",
    "                 'dropout': self.dropout,\n",
    "                 'nhiddens': self.nhiddens,\n",
    "                 'nlatent': self.nlatent,\n",
    "                 'state': self.state_dict(),\n",
    "                }\n",
    "\n",
    "        _torch.save(state, filehandle)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, cuda=False, evaluate=True):\n",
    "        \"\"\"Instantiates a VAE from a model file.\n",
    "        Inputs:\n",
    "            path: Path to model file as created by functions VAE.save or\n",
    "                  VAE.trainmodel.\n",
    "            cuda: If network should work on GPU [False]\n",
    "            evaluate: Return network in evaluation mode [True]\n",
    "        Output: VAE with weights and parameters matching the saved network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Forcably load to CPU even if model was saves as GPU model\n",
    "        dictionary = _torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        nsamples = dictionary['nsamples']\n",
    "        alpha = dictionary['alpha']\n",
    "        beta = dictionary['beta']\n",
    "        dropout = dictionary['dropout']\n",
    "        nhiddens = dictionary['nhiddens']\n",
    "        nlatent = dictionary['nlatent']\n",
    "        state = dictionary['state']\n",
    "\n",
    "        vae = cls(nsamples, nhiddens, nlatent, alpha, beta, dropout, cuda)\n",
    "        vae.load_state_dict(state)\n",
    "\n",
    "        if cuda:\n",
    "            vae.cuda()\n",
    "\n",
    "        if evaluate:\n",
    "            vae.eval()\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def trainmodel(self, dataloader, nepochs=500, lrate=1e-3,\n",
    "                   batchsteps=[25, 75, 150, 300], logfile=None, modelfile=None):\n",
    "        \"\"\"Train the autoencoder from depths array and tnf array.\n",
    "        Inputs:\n",
    "            dataloader: DataLoader made by make_dataloader\n",
    "            nepochs: Train for this many epochs before encoding [500]\n",
    "            lrate: Starting learning rate for the optimizer [0.001]\n",
    "            batchsteps: None or double batchsize at these epochs [25, 75, 150, 300]\n",
    "            logfile: Print status updates to this file if not None [None]\n",
    "            modelfile: Save models to this file if not None [None]\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "\n",
    "        if lrate < 0:\n",
    "            raise ValueError('Learning rate must be positive, not {}'.format(lrate))\n",
    "\n",
    "        if nepochs < 1:\n",
    "            raise ValueError('Minimum 1 epoch, not {}'.format(nepochs))\n",
    "\n",
    "        if batchsteps is None:\n",
    "            batchsteps_set = set()\n",
    "        else:\n",
    "            # First collect to list in order to allow all element types, then check that\n",
    "            # they are integers\n",
    "            batchsteps = list(batchsteps)\n",
    "            if not all(isinstance(i, int) for i in batchsteps):\n",
    "                raise ValueError('All elements of batchsteps must be integers')\n",
    "            if max(batchsteps, default=0) >= nepochs:\n",
    "                raise ValueError('Max batchsteps must not equal or exceed nepochs')\n",
    "            last_batchsize = dataloader.batch_size * 2**len(batchsteps)\n",
    "            if len(dataloader.dataset) < last_batchsize:\n",
    "                raise ValueError('Last batch size exceeds dataset length')\n",
    "            batchsteps_set = set(batchsteps)\n",
    "\n",
    "        # Get number of features\n",
    "        ncontigs, nsamples = dataloader.dataset.tensors[0].shape\n",
    "        optimizer = _Adam(self.parameters(), lr=lrate)\n",
    "\n",
    "        if logfile is not None:\n",
    "            print('\\tNetwork properties:', file=logfile)\n",
    "            print('\\tCUDA:', self.usecuda, file=logfile)\n",
    "            print('\\tAlpha:', self.alpha, file=logfile)\n",
    "            print('\\tBeta:', self.beta, file=logfile)\n",
    "            print('\\tDropout:', self.dropout, file=logfile)\n",
    "            print('\\tN hidden:', ', '.join(map(str, self.nhiddens)), file=logfile)\n",
    "            print('\\tN latent:', self.nlatent, file=logfile)\n",
    "            print('\\n\\tTraining properties:', file=logfile)\n",
    "            print('\\tN epochs:', nepochs, file=logfile)\n",
    "            print('\\tStarting batch size:', dataloader.batch_size, file=logfile)\n",
    "            batchsteps_string = ', '.join(map(str, sorted(batchsteps))) if batchsteps_set else \"None\"\n",
    "            print('\\tBatchsteps:', batchsteps_string, file=logfile)\n",
    "            print('\\tLearning rate:', lrate, file=logfile)\n",
    "            print('\\tN sequences:', ncontigs, file=logfile)\n",
    "            print('\\tN samples:', nsamples, file=logfile, end='\\n\\n')\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(nepochs):\n",
    "            dataloader = self.trainepoch(dataloader, epoch, optimizer, batchsteps_set, logfile)\n",
    "\n",
    "        # Save weights - Lord forgive me, for I have sinned when catching all exceptions\n",
    "        if modelfile is not None:\n",
    "            try:\n",
    "                self.save(modelfile)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNF = _np.loadtxt('/home/maog/data/metahit/tnfs.txt').astype(_np.float32)\n",
    "RPKM = _np.load('/home/maog/data/metahit/abundance.npz')\n",
    "vae = VAE(nsamples=RPKM['arr_0'].shape[1], cuda=True)\n",
    "dataloader, mask = make_dataloader(RPKM['arr_0'], TNF)\n",
    "vae.trainmodel(dataloader)\n",
    "latent = vae.encode(dataloader)\n",
    "print(latent.shape)\n",
    "_np.savetxt('/home/maog/data/metahit/vMF-VAE_latent.txt', latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = _np.loadtxt('/home/maog/data/metahit/vMF-VAE_latent.txt').astype(_np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix, inplace=False):\n",
    "    if isinstance(matrix, np.ndarray):\n",
    "        matrix = torch.from_numpy(matrix)\n",
    "\n",
    "    matrix = matrix.clone()\n",
    "\n",
    "    # If any rows are kept all zeros, the distance function will return 0.5 to all points\n",
    "    # inclusive itself, which can break the code in this module\n",
    "    zeromask = matrix.sum(dim=1) == 0\n",
    "    matrix[zeromask] = 1/matrix.shape[1]\n",
    "    matrix /= (matrix.norm(dim=1).reshape(-1, 1) * (2 ** 0.5))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = normalize(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = cluster_points(latent, windowsize = 25, radius = 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "contig_length = {}\n",
    "contig_id_idx = {}\n",
    "contig_idx_id = {}\n",
    "contigs = '/home/maog/data/metahit/contigs.fna'\n",
    "for record in SeqIO.parse(contigs, \"fasta\"):\n",
    "    contig_length[record.id] = len(record.seq)\n",
    "    contig_idx_id[len(contig_id_idx)] = record.id\n",
    "    contig_id_idx[record.id] = len(contig_id_idx)\n",
    "filtered_bins, cluster_contig_id = filterclusters(clusters, contig_length, contig_idx_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# y_pred = DBSCAN(eps = 0.17, min_samples = 50, metric = 'cosine').fit_predict(latent)\n",
    "# import collections\n",
    "# #clusters = dict()\n",
    "# clusters = collections.defaultdict(set)\n",
    "# for i in range(len(y_pred)):\n",
    "#     clusters[y_pred[i]].add(i)\n",
    "# len(clusters.keys())\n",
    "# from Bio import SeqIO\n",
    "# contig_length = {}\n",
    "# contig_id_idx = {}\n",
    "# contig_idx_id = {}\n",
    "# contigs = '/home/maog/data/skin/contigs.fna'\n",
    "# for record in SeqIO.parse(contigs, \"fasta\"):\n",
    "#     contig_length[record.id] = len(record.seq)\n",
    "#     contig_idx_id[len(contig_id_idx)] = record.id\n",
    "#     contig_id_idx[record.id] = len(contig_id_idx)\n",
    "# def filterclusters(clusters, lengthof, contig_idx_id):\n",
    "#     filtered_bins = dict()\n",
    "#     cluster_contig_id = []\n",
    "#     for medoid_id, contigs_id in clusters.items():\n",
    "#         binsize = sum(lengthof[contig_idx_id[int(contig_id)]] for contig_id in contigs_id)\n",
    "#         if binsize >= 200000:\n",
    "#             filtered_bins[medoid_id] = contigs_id\n",
    "#             for contig_id in contigs_id:\n",
    "#                 cluster_contig_id.append(contig_id)\n",
    "\n",
    "#     return filtered_bins, cluster_contig_id\n",
    "# filtered_bins, _ = filterclusters(clusters, contig_length, contig_idx_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "cluster = dict()\n",
    "cluster = collections.defaultdict(set)\n",
    "for k, v in filtered_bins.items():\n",
    "    for i in v:\n",
    "        if k != -1:\n",
    "            cluster[\"bins\"+ str(k)].add(contig_idx_id[i])\n",
    "len(cluster.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load in the Reference\n",
    "reference_path = '/home/maog/data/airways/reference.tsv'\n",
    "\n",
    "!head $reference_path # show first 10 lines of reference file\n",
    "\n",
    "with open(reference_path) as reference_file:\n",
    "    reference = DRBin.benchmark.Reference.from_file(reference_file)\n",
    "    \n",
    "taxonomy_path = '/home/maog/data/airways/taxonomy.tsv'\n",
    "\n",
    "!head $taxonomy_path # show first 10 lines of reference file\n",
    "\n",
    "with open(taxonomy_path) as taxonomy_file:\n",
    "    reference.load_tax_file(taxonomy_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
